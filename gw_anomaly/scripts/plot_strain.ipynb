{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from gwpy.timeseries import TimeSeries\n",
    "from astropy import units as u\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.grid'] = False\n",
    "from matplotlib.colors import LinearSegmentedColormap, TwoSlopeNorm\n",
    "from models import LinearModel, GwakClassifier\n",
    "from evaluate_data import full_evaluation\n",
    "import json\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import conv1d\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import welch\n",
    "import pickle\n",
    "from config import (\n",
    "    CHANNEL,\n",
    "    GPU_NAME,\n",
    "    SEGMENT_OVERLAP,\n",
    "    SAMPLE_RATE,\n",
    "    BANDPASS_HIGH,\n",
    "    BANDPASS_LOW,\n",
    "    FACTORS_NOT_USED_FOR_FM,\n",
    "    MODELS_LOCATION,\n",
    "    SEG_NUM_TIMESTEPS,\n",
    "    CLASS_ORDER\n",
    "    )\n",
    "from helper_functions import (\n",
    "    far_to_metric, \n",
    "    compute_fars, \n",
    "    load_gwak_models, \n",
    "    joint_heuristic_test, \n",
    "    combine_freqcorr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "GPU_NAME = 'cuda:1'\n",
    "DEVICE = torch.device(GPU_NAME)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasedModel, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(3, 1)\n",
    "        self.layer2_1 = nn.Linear(1, 1)\n",
    "        self.layer2_2 = nn.Linear(1, 1)\n",
    "        self.layer2_3 = nn.Linear(1, 1)\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.activation(self.layer1(x[:, :3]))\n",
    "        x2_1 = self.activation(self.layer2_1(x[:, 3:4]))\n",
    "        x2_2 = self.activation(self.layer2_1(x[:, 4:5]))\n",
    "        x2_3 = self.activation(self.layer2_1(x[:, 5:6]))\n",
    "        return x1 * x2_1 * x2_2 * x2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(gwak_values):\n",
    "    result = np.zeros((gwak_values.shape[0], 3))\n",
    "    for i, pair in enumerate([[3, 4], [9, 10], [12, 13]]):\n",
    "        a, b = pair\n",
    "        ratio_a = (np.abs(gwak_values[:, a]) + 2) / (np.abs(gwak_values[:, b]) + 2)\n",
    "        ratio_b = (np.abs(gwak_values[:, b]) + 2) / (np.abs(gwak_values[:, a]) + 2)\n",
    "\n",
    "        ratio = np.maximum(ratio_a, ratio_b)\n",
    "        result[:, i] = ratio\n",
    "    return result\n",
    "\n",
    "def compute_signal_strength_chop_sep(x, y):\n",
    "    psd0 = welch(x)[1]\n",
    "    psd1 = welch(y)[1]\n",
    "    HLS = np.log(np.sum(psd0))\n",
    "    LLS = np.log(np.sum(psd1))\n",
    "    return HLS, LLS\n",
    "    \n",
    "def shifted_pearson(H, L, H_start, H_end, maxshift=int(10*4096/1000)):\n",
    "    # works for one window at a time\n",
    "    Hs = H[H_start:H_end]\n",
    "    minval = 1\n",
    "    for shift in range(-maxshift, maxshift):\n",
    "        Ls = L[H_start+shift:H_end+shift]\n",
    "        #minval = min(pearsonr(Hs, Ls)[0], minval)\n",
    "        p = pearsonr(Hs, Ls)[0]\n",
    "        if p < minval:\n",
    "            minval = p\n",
    "            shift_idx = shift\n",
    "\n",
    "    return minval, shift_idx\n",
    "\n",
    "def parse_strain(x):\n",
    "    # take strain, compute the long sig strenght & pearson\n",
    "    # split it up, do the same thing for short\n",
    "    long_pearson, shift_idx = shifted_pearson(x[0], x[1], 50, len(x[0])-50)\n",
    "    #long_sig_strength = compute_signal_strength_chop(x[0, 50:-50], x[1, 50+shift_idx:len(x[0])-50+shift_idx] )\n",
    "    HSS, LSS = compute_signal_strength_chop_sep(x[0, 50:-50], x[1, 50+shift_idx:len(x[0])-50+shift_idx])\n",
    "    return long_pearson, HSS, LSS\n",
    "\n",
    "def parse_gwtc_catalog(path, mingps=None, maxgps=None):\n",
    "    gwtc = np.loadtxt(path, delimiter=\",\", dtype=\"str\")\n",
    "\n",
    "    pulled_data = np.zeros((gwtc.shape[0]-1, 3))\n",
    "    for i, elem in enumerate(gwtc[1:]): #first row is just data value description\n",
    "        pulled_data[i] = [float(elem[4]), float(elem[13]), float(elem[34])]\n",
    "\n",
    "    if mingps != None:\n",
    "        assert maxgps != None\n",
    "        pulled_data = pulled_data[np.logical_and(pulled_data[:, 0]<maxgps, pulled_data[:, 0]>mingps)]\n",
    "    return pulled_data\n",
    "\n",
    "def find_segment(gps, segs):\n",
    "    for seg in segs:\n",
    "        a, b = seg\n",
    "        if a < gps and b > gps:\n",
    "            return seg\n",
    "\n",
    "def sig_prob_function(evals, scale=40):\n",
    "    sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "    #sigmoid = lambda x: 1/(1+np.exp(-(x-0.3)))\n",
    "    return 1-(sigmoid(scale * (evals-0.5)))\n",
    "\n",
    "def get_far(score, sort_eval):\n",
    "    ind = np.searchsorted(sort_eval, score)\n",
    "    if ind == len(sort_eval):\n",
    "        ind -= 1\n",
    "    #N = len(sort_eval)\n",
    "    units = 10000*3.15e7\n",
    "    return ind/units\n",
    "\n",
    "def make_eval_chunks(a, b, dur):\n",
    "    '''\n",
    "    Split up into one-hour chunks to normalize the whitening duration\n",
    "    a, b - ints\n",
    "    A, B - strings\n",
    "\n",
    "    output - only care about the strings\n",
    "    '''\n",
    "    n_full_chunks = (b-a)//dur\n",
    "\n",
    "    out = []\n",
    "    for n in range(1, n_full_chunks+1):\n",
    "        out.append([str(a+(n-1)*dur), str(a+n*dur)])\n",
    "\n",
    "    #ending chunk, but still make it one hour\n",
    "    out.append([str(b-dur), str(b)])\n",
    "    return out\n",
    "\n",
    "def event_clustering(indices, scores, spacing, device):\n",
    "    '''\n",
    "    Group the evaluations into events, i.e. treat a consecutive sequence\n",
    "    of low anomaly score as a single event\n",
    "    '''\n",
    "\n",
    "    clustered = []\n",
    "    idxs = indices.detach().cpu().numpy()\n",
    "    cluster = []\n",
    "    for i, elem in enumerate(idxs):\n",
    "        # to move onto next cluster\n",
    "        if i != 0:\n",
    "            dist = elem - idxs[i-1]\n",
    "            if dist > spacing:\n",
    "                #make a new cluster\n",
    "                clustered.append(cluster)\n",
    "                cluster = [] # and initiate a new one\n",
    "        cluster.append(elem)\n",
    "    clustered.append(cluster) # last one isn't captured, since we haven't moved on\n",
    "    final_points = []\n",
    "    for cluster in clustered:\n",
    "        # take the one with the lowest score (most significant)\n",
    "        bestscore = 10\n",
    "        bestval = None\n",
    "        for elem in cluster:\n",
    "            if scores[elem] < bestscore:\n",
    "                bestscore = scores[elem]\n",
    "                bestval = elem\n",
    "        final_points.append(bestval)\n",
    "    return torch.from_numpy(np.array(final_points)).int().to(device)\n",
    "\n",
    "def extract_chunks(strain_data, timeslide_num, important_points, device,\n",
    "                    roll_amount = SEG_NUM_TIMESTEPS, window_size=1024):\n",
    "    '''\n",
    "    Important points are indicies into thestrain_data\n",
    "    '''\n",
    "    L_shift = timeslide_num*roll_amount\n",
    "    timeslide_len = strain_data.shape[1]\n",
    "    edge_check_passed = []\n",
    "    fill_strains = np.zeros((len(important_points), 2, window_size*2))\n",
    "    for idx, point in enumerate(important_points):\n",
    "        # check that the point is not on the edge\n",
    "        edge_check_passed.append(not(point < window_size * 2 or timeslide_len - point < window_size*2))\n",
    "        if not(point < window_size * 2 or timeslide_len - point < window_size*2):\n",
    "            H_selection = strain_data[0, point-window_size:point+window_size]\n",
    "\n",
    "            # if the livingston points overflow, the modulo should bring them\n",
    "            # into the right location. also data is clipped //1000 * 1000\n",
    "            # which is divisible by 200, so it should work\n",
    "            L_start = (point-window_size+L_shift) % timeslide_len\n",
    "            L_end = (point+window_size+L_shift) % timeslide_len\n",
    "\n",
    "            L_selection = strain_data[1, L_start:L_end]\n",
    "\n",
    "            fill_strains[idx, 0, :] = H_selection\n",
    "            fill_strains[idx, 1, :] = L_selection\n",
    "\n",
    "    return fill_strains, edge_check_passed\n",
    "\n",
    "def whiten_bandpass_resample(\n",
    "        start_point,\n",
    "        end_point,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        bandpass_low=BANDPASS_LOW,\n",
    "        bandpass_high=BANDPASS_HIGH,\n",
    "        shift=None):\n",
    "\n",
    "    device = torch.device(GPU_NAME)\n",
    "\n",
    "    start_point, end_point = int(start_point)-10, int(end_point)-10\n",
    "    strainL1 = TimeSeries.fetch_open_data(\"L1\", start_point, end_point)\n",
    "    strainH1 = TimeSeries.fetch_open_data(\"H1\", start_point, end_point) #f'H1:{CHANNEL}',\n",
    "\n",
    "    strainL1 = strainL1.resample(sample_rate).bandpass(bandpass_low, bandpass_high)\n",
    "    strainL1 = strainL1.whiten()\n",
    "\n",
    "    strainH1 = strainH1.resample(sample_rate).bandpass(bandpass_low, bandpass_high)\n",
    "    strainH1 = strainH1.whiten()\n",
    "\n",
    "    return [strainH1, strainL1]\n",
    "\n",
    "def whiten_bandpass_resample_new_order(\n",
    "        start_point,\n",
    "        end_point,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        bandpass_low=BANDPASS_LOW,\n",
    "        bandpass_high=BANDPASS_HIGH,\n",
    "        shift=None):\n",
    "\n",
    "    device = torch.device(GPU_NAME)\n",
    "\n",
    "    start_point, end_point = int(start_point)-10, int(end_point)-10\n",
    "    strainL1_0 = TimeSeries.fetch_open_data(\"L1\", start_point, end_point)#f'L1:{CHANNEL}',\n",
    "    strainH1_0 = TimeSeries.fetch_open_data(\"H1\", start_point, end_point)#f'H1:{CHANNEL}',\n",
    "\n",
    "    strainL1 = strainL1_0.bandpass(bandpass_low, bandpass_high).whiten()#.resample(sample_rate).whiten()\n",
    "    strainH1 = strainH1_0.bandpass(bandpass_low, bandpass_high).whiten()#.resample(sample_rate).whiten()\n",
    "\n",
    "    return [strainH1, strainL1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evals(data_, model_path, savedir, start_point,\n",
    "              gwpy_timeseries, neworder_clean=None, neworder_raw=None,\n",
    "              manual_eval_times=None, metric=None):\n",
    "    heur_model_path = \"/home/katya.govorkova/gwak-paper-final-models/trained/model_heuristic.h5\"\n",
    "    model_heuristic = BasedModel().to(DEVICE)\n",
    "    model_heuristic.load_state_dict(torch.load(heur_model_path, map_location=DEVICE))\n",
    "\n",
    "    # split the data into 1-hour chunks to fit in memory best\n",
    "    eval_at_once_len = int(3600)\n",
    "    N_one_hour_splits = int(data_.shape[1]//(eval_at_once_len*SAMPLE_RATE) + 1)\n",
    "    print(\"N splits:\", N_one_hour_splits)\n",
    "\n",
    "    for hour_split in range(N_one_hour_splits):\n",
    "        start = int(hour_split*SAMPLE_RATE*eval_at_once_len)\n",
    "        end = int(min(data_.shape[1], (hour_split+1)*SAMPLE_RATE*eval_at_once_len))\n",
    "        print(start, end)\n",
    "        if end - 10 < start:\n",
    "            return None\n",
    "        data = data_[:, start:end]\n",
    "\n",
    "        model_types = [\"bbh.pt\",\n",
    "                       \"sglf.pt\",\n",
    "                       \"sghf.pt\",\n",
    "                       \"background.pt\",\n",
    "                       \"glitches.pt\"]\n",
    "\n",
    "        model_paths = []\n",
    "        for elem in model_types:\n",
    "            model_paths.append(model_path + elem)\n",
    "\n",
    "        gwak_models = load_gwak_models(model_paths, DEVICE, GPU_NAME)\n",
    "\n",
    "        norm_factors = np.load(f\"/home/katya.govorkova/gwak-paper-final-models/trained/norm_factor_params.npy\")\n",
    "        fm_model_path = (\"/home/katya.govorkova/gwak-paper-final-models/trained/fm_model.pt\")\n",
    "\n",
    "        orig_kernel = 50\n",
    "        kernel_len = int(orig_kernel * 5/SEGMENT_OVERLAP)\n",
    "        kernel = torch.ones((1, kernel_len)).float().to(DEVICE)/kernel_len\n",
    "        kernel = kernel[None, :, :]\n",
    "        heuristics_tests = True\n",
    "\n",
    "        fm_model = LinearModel(21-len(FACTORS_NOT_USED_FOR_FM)).to(DEVICE)\n",
    "        fm_model.load_state_dict(torch.load(\n",
    "            fm_model_path, map_location=GPU_NAME))\n",
    "\n",
    "        linear_weights = fm_model.layer.weight.detach()\n",
    "        bias_value = fm_model.layer.bias.detach()\n",
    "        linear_weights[:, -2] += linear_weights[:, -1]\n",
    "        linear_weights = linear_weights[:, :-1]\n",
    "        norm_factors = norm_factors[:, :-1]\n",
    "\n",
    "        orig_kernel = 50\n",
    "        kernel_len = int(orig_kernel * 5/SEGMENT_OVERLAP)\n",
    "        kernel = torch.ones((1, kernel_len)).float().to(DEVICE)/kernel_len\n",
    "        kernel = kernel[None, :, :]\n",
    "        heuristics_tests = True\n",
    "\n",
    "        mean_norm = torch.from_numpy(norm_factors[0]).to(DEVICE)\n",
    "        std_norm = torch.from_numpy(norm_factors[1]).to(DEVICE)\n",
    "\n",
    "        final_values, midpoints, original, recreated = full_evaluation(\n",
    "                        data[None, :, :], model_paths, DEVICE,\n",
    "                        return_midpoints=True, return_recreations=True,\n",
    "                        loaded_models=gwak_models, grad_flag=False)\n",
    "\n",
    "        final_values = final_values[0]\n",
    "\n",
    "        # Set the threshold here\n",
    "        FAR_2days = -1 # lowest FAR bin we want to worry about\n",
    "\n",
    "        # Inference to save scores (final metric) and scaled_evals (GWAK space * weights unsummed)\n",
    "        final_values_slx = (final_values - mean_norm)/std_norm\n",
    "        scaled_evals = torch.multiply(final_values_slx, linear_weights[None, :])[0, :]\n",
    "\n",
    "        scores = (scaled_evals.sum(axis=1) + bias_value)[:, None]\n",
    "        scaled_evals = conv1d(scaled_evals.transpose(0, 1).float()[:, None, :],\n",
    "            kernel, padding=\"same\").transpose(0, 1)[0].transpose(0, 1)\n",
    "        smoothed_scores = conv1d(scores.transpose(0, 1).float()[:, None, :],\n",
    "            kernel, padding=\"same\").transpose(0, 1)[0].transpose(0, 1)\n",
    "        indices = torch.where(smoothed_scores < FAR_2days)[0]\n",
    "\n",
    "        manual_eval_times = torch.tensor(manual_eval_times).to(indices.device)\n",
    "        if manual_eval_times[0] > start_point:\n",
    "            manual_eval_times -= start_point\n",
    "\n",
    "        manual_eval_indices = torch.zeros_like(manual_eval_times)\n",
    "\n",
    "        for i, eval_time in enumerate(manual_eval_times):\n",
    "            eval_time = eval_time * SAMPLE_RATE\n",
    "            insert_location = torch.searchsorted(torch.from_numpy(midpoints).to(eval_time.device), eval_time)\n",
    "            print(414, torch.searchsorted(torch.from_numpy(midpoints).to(eval_time.device), eval_time))\n",
    "            manual_eval_indices[i] = insert_location\n",
    "\n",
    "\n",
    "\n",
    "        if len(indices) == 0: continue # Didn't find anything\n",
    "\n",
    "        indices = event_clustering(indices, smoothed_scores, 5*SAMPLE_RATE/SEGMENT_OVERLAP, DEVICE) # 5 seconds\n",
    "        indices = torch.cat([indices, manual_eval_indices]).int()\n",
    "        filtered_final_score = smoothed_scores.index_select(0, indices)\n",
    "        filtered_final_scaled_evals = scaled_evals.index_select(0, indices)\n",
    "\n",
    "        indices = indices.detach().cpu().numpy()\n",
    "        # extract important \"events\" with indices\n",
    "        timeslide_chunks, edge_check_filter = extract_chunks(data, 0, # 0 - timeslide number 0 (no shifting happening)\n",
    "                                            midpoints[indices],\n",
    "                                            DEVICE, window_size=1024) # 0.25 seconds on either side\n",
    "                                                                    # so it should come out to desired 0.5\n",
    "\n",
    "        filtered_final_scaled_evals = filtered_final_scaled_evals.detach().cpu().numpy()\n",
    "        filtered_final_score = filtered_final_score.detach().cpu().numpy()\n",
    "\n",
    "        filtered_final_scaled_evals = filtered_final_scaled_evals[edge_check_filter]\n",
    "        filtered_final_score = filtered_final_score[edge_check_filter]\n",
    "        timeslide_chunks = timeslide_chunks[edge_check_filter]\n",
    "        indices = indices[edge_check_filter]\n",
    "\n",
    "        filtered_timeslide_chunks = timeslide_chunks\n",
    "\n",
    "        heuristics_tests = True\n",
    "        if heuristics_tests:\n",
    "            N_initial = len(filtered_final_score)\n",
    "            passed_heuristics = []\n",
    "            gwak_filtered = extract(filtered_final_scaled_evals)\n",
    "            for i, strain_segment in enumerate(timeslide_chunks):\n",
    "                strain_feats = parse_strain(strain_segment)\n",
    "                together = np.concatenate([strain_feats, gwak_filtered[i]])\n",
    "                print(433, \"together\", together, filtered_final_score[i])\n",
    "                res = model_heuristic(torch.from_numpy(together[None, :]).float().to(DEVICE)).item()\n",
    "                #passed_heuristics.append(res<0.46)\n",
    "                #res -= 0.1\n",
    "\n",
    "                res_sigmoid = sig_prob_function(res)\n",
    "                #print(res, res_sigmoid, filtered_final_score[i])\n",
    "                final_final = res_sigmoid * filtered_final_score[i]\n",
    "                #print(res, res_sigmoid, filtered_final_score[i])\n",
    "                filtered_final_score[i] *= res_sigmoid\n",
    "                print(res, res_sigmoid, filtered_final_score[i])\n",
    "                passed_heuristics.append(final_final[0] < -1.75) #[0] since it was saving arrays(arrays)\n",
    "\n",
    "\n",
    "            filtered_final_scaled_evals = filtered_final_scaled_evals[passed_heuristics]\n",
    "            filtered_final_score = filtered_final_score[passed_heuristics]\n",
    "            filtered_timeslide_chunks = timeslide_chunks[passed_heuristics]\n",
    "            indices = indices[passed_heuristics]\n",
    "\n",
    "            print(f\"Fraction removed by heuristics test {N_initial -len(filtered_final_score)}/{N_initial}\")\n",
    "        # rename them for less confusion, easier typing\n",
    "        gwak_values = filtered_final_scaled_evals\n",
    "        fm_scores = filtered_final_score\n",
    "        strain_chunks = filtered_timeslide_chunks\n",
    "\n",
    "        if strain_chunks.shape[0] == 0: continue\n",
    "        # plotting all these significant events\n",
    "        n_points = strain_chunks.shape[2]\n",
    "\n",
    "        scaled_evals = scaled_evals.cpu().numpy()\n",
    "        scaled_evals = combine_freqcorr(scaled_evals)\n",
    "        bias_value = bias_value.cpu().numpy()\n",
    "        smoothed_scores = smoothed_scores.cpu().numpy()\n",
    "\n",
    "        # for j in range(len(gwak_values)):\n",
    "        #     indiv_fig = np.empty((2, 2), dtype=object)\n",
    "        #     indiv_axs = np.empty((2, 2), dtype=object)\n",
    "        #     for a in range(2):\n",
    "\n",
    "        #         for b in range(2):\n",
    "        #             temp_fig, temp_axs = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "        #             indiv_fig[a, b] = temp_fig\n",
    "        #             indiv_axs[a, b] = temp_axs\n",
    "\n",
    "        #     loudest = indices[j]\n",
    "        #     left_edge = 1024 //SEGMENT_OVERLAP\n",
    "        #     right_edge = 1024 // SEGMENT_OVERLAP\n",
    "        #     quak_evals_ts = np.linspace(0, (left_edge+right_edge)*SEGMENT_OVERLAP/SAMPLE_RATE  , left_edge+right_edge)\n",
    "        #     labels = ['background','background', 'bbh','bbh', 'glitch', 'glitch', 'sglf', 'sglf', 'sghf', 'sghf', 'freq corr']\n",
    "        #     cols = [\n",
    "        #         \"#6c5b7b\",  # Background (Slate Blue)\n",
    "        #         \"#f29e4c\",  # BBH (Amber)\n",
    "        #         \"#81b29a\",  # Glitch (Teal)\n",
    "        #         \"#708090\",  # SGLF (Light Slate Gray)\n",
    "        #         \"#00bfff\",  # SGHF (Deep Sky Blue)\n",
    "        #         \"#cd5c5c\",  # Freq Corr (Indian Red)\n",
    "        #         \"#006400\",  # Final Metric (Dark Green)\n",
    "        #         \"#daa520\",  # Hanford (Goldenrod)\n",
    "        #         \"#ff6347\",  # Livingston (Tomato)\n",
    "        #     ]\n",
    "        #     for i in range(scaled_evals.shape[1]):\n",
    "        #         line_type = \"-\"\n",
    "        #         if i% 2 == 1:\n",
    "        #             line_type = \"--\"\n",
    "        #         if i % 2 == 0 or labels[i] in [\"freq corr\"]:\n",
    "\n",
    "        #             indiv_axs[1, 0].plot(1000*quak_evals_ts, scaled_evals[loudest-left_edge:loudest+right_edge, i],\n",
    "        #                         label = labels[i], c=cols[i//2], linestyle=line_type)\n",
    "\n",
    "        #         else:\n",
    "        #             indiv_axs[1, 0].plot(1000*quak_evals_ts, scaled_evals[loudest-left_edge:loudest+right_edge, i],\n",
    "        #                             c=cols[i//2], linestyle=line_type)\n",
    "\n",
    "\n",
    "        #     indiv_axs[1, 0].plot(1000*quak_evals_ts, smoothed_scores[loudest-left_edge:loudest+right_edge]-bias_value, label = 'final metric', c='black')\n",
    "        #     indiv_axs[1, 0].plot([], [], '-', label=\"Hanford\", c=\"black\")\n",
    "        #     indiv_axs[1, 0].plot([], [], '--', label=\"Livingston\", c=\"black\")\n",
    "        #     indiv_axs[1, 0].legend(handlelength=3, fontsize=17)\n",
    "        #     indiv_axs[1, 0].set_xlabel(\"Time, (ms)\")\n",
    "        #     indiv_axs[1, 0].set_ylabel(\"Final Metric Contribution\")\n",
    "\n",
    "        #     strain_ts = np.linspace(0, len(strain_chunks[j, 0, :])/SAMPLE_RATE, len(strain_chunks[j, 0, :]))\n",
    "        #     indiv_axs[0, 0].plot(strain_ts, strain_chunks[j, 0, :], label = 'Hanford', alpha=0.8, c=\"#6c5b7b\")\n",
    "        #     indiv_axs[0, 0].plot(strain_ts, strain_chunks[j, 1, :], label = 'Livingston', alpha=0.8, c=\"#f29e4c\")\n",
    "        #     indiv_axs[0, 0].set_xlabel('Time, (ms)')\n",
    "        #     indiv_axs[0, 0].set_ylabel('strain')\n",
    "        #     indiv_axs[0, 0].legend()\n",
    "        #     indiv_axs[0, 0].set_title(f'gps time: {start_point + midpoints[loudest]/SAMPLE_RATE + hour_split*eval_at_once_len:.2f}')\n",
    "        #     p = midpoints[loudest]\n",
    "\n",
    "        #     do_q_scan = True\n",
    "        #     if do_q_scan:\n",
    "        #         # plot the Q-scans\n",
    "        #         left_edge = 1024\n",
    "        #         right_edge = 1024\n",
    "        #         q_edge = int(7.5*4096)\n",
    "        #         H_strain = gwpy_timeseries[0][p-left_edge-q_edge + eval_at_once_len*hour_split*SAMPLE_RATE:p+right_edge+q_edge + eval_at_once_len*hour_split*SAMPLE_RATE]\n",
    "        #         L_strain = gwpy_timeseries[1][p-left_edge-q_edge + eval_at_once_len*hour_split*SAMPLE_RATE:p+right_edge+q_edge + eval_at_once_len*hour_split*SAMPLE_RATE]\n",
    "        #         t0 = H_strain.t0.value\n",
    "        #         dt = H_strain.dt.value\n",
    "\n",
    "        #         H_hq = H_strain.q_transform(outseg=(t0+q_edge*dt, t0+q_edge*dt+(left_edge+right_edge)*dt), whiten=False)\n",
    "        #         L_hq = L_strain.q_transform(outseg=(t0+q_edge*dt, t0+q_edge*dt+(left_edge+right_edge)*dt), whiten=False)\n",
    "        #         f = np.array(H_hq.yindex)\n",
    "        #         t = np.array(H_hq.xindex)\n",
    "        #         #t=strain_ts *1000\n",
    "        #         t -= t[0]\n",
    "\n",
    "        #         im_H = indiv_axs[0, 1].pcolormesh(t*1000, f, np.array(H_hq).T, vmax = 25, vmin = 0)\n",
    "        #         indiv_fig[0, 1].colorbar(im_H, ax=indiv_axs[0, 1], label = \"spectral power\")\n",
    "        #         indiv_axs[0, 1].set_yscale(\"log\")\n",
    "        #         indiv_axs[0, 1].set_xlabel(\"Time (ms)\")\n",
    "        #         indiv_axs[0, 1].set_ylabel(\"Freq (Hz)\")\n",
    "        #         indiv_axs[0, 1].set_title(\"Hanford Q-Transform\")\n",
    "\n",
    "        #         im_L  = indiv_axs[1, 1].pcolormesh(t*1000, f, np.array(L_hq).T, vmax = 25, vmin = 0)\n",
    "        #         indiv_fig[1, 1].colorbar(im_L, ax=indiv_axs[1, 1], label = \"spectral power\")\n",
    "        #         indiv_axs[1, 1].set_yscale(\"log\")\n",
    "        #         indiv_axs[1, 1].set_xlabel(\"Time (ms)\")\n",
    "        #         indiv_axs[1, 1].set_ylabel(\"Freq (Hz)\")\n",
    "        #         indiv_axs[1, 1].set_title(\"Livingston Q-Transform\")\n",
    "\n",
    "        #     best_far = 0\n",
    "        #     best_score = fm_scores[j][0] # [ [one element], [one element]] structure\n",
    "        #     print(\"best_score\", best_score)\n",
    "        #     base = f'{savedir}/{start_point+p/SAMPLE_RATE:.3f}_{best_score:.2f}'\n",
    "        #     plt.savefig(f'{base}.png', dpi=300, bbox_inches=\"tight\")\n",
    "        #     rename_map = np.array([[\"strain\", \"H_qtransform\"],[\"gwak_values\", \"L_qtransform\"]])\n",
    "        #     for a in range(2):\n",
    "        #         for b in range(2):\n",
    "        #             indiv_fig[a, b].savefig(f\"{base}_{rename_map[a, b]}.png\", dpi=200)\n",
    "        #             plt.close(indiv_fig[a, b])\n",
    "        for j in range(len(gwak_values)):\n",
    "            # Create two figures\n",
    "            fig1, axs1 = plt.subplots(4, 1, figsize=(10, 12) ) #, sharex=True)  # Strain and GWAK values (shared x-axis)\n",
    "            # fig2, axs2 = plt.subplots(2, 1, figsize=(10, 8), sharex=True, constrained_layout=True)  # Hanford and Livingston Q-transforms (shared y-axis)\n",
    "        \n",
    "            loudest = indices[j]\n",
    "            left_edge = 1024 // SEGMENT_OVERLAP\n",
    "            right_edge = 1024 // SEGMENT_OVERLAP\n",
    "            quak_evals_ts = np.linspace(0, (left_edge + right_edge) * SEGMENT_OVERLAP / SAMPLE_RATE, left_edge + right_edge)\n",
    "        \n",
    "            labels = ['Background', 'Background', 'BBH', 'BBH', 'Glitch', 'Glitch', 'SG (64-512 Hz)', 'SG (64-512 Hz)', 'SG (512-1024 Hz)', 'SG (512-1024 Hz)', 'Frequency correlation']\n",
    "            cols = [\n",
    "                \"#f4a3c1\",  # Background (Soft Pink)\n",
    "                \"#ffd700\",  # BBH (Yellow - Gold)\n",
    "                \"#2a9d8f\",  # Glitch (Emerald Green)\n",
    "                \"#708090\",  # SGLF (Light Slate Gray)\n",
    "                \"#00bfff\",  # SGHF (Deep Sky Blue)\n",
    "                \"#cd5c5c\",  # Freq Corr (Indian Red)\n",
    "                \"#006400\",  # Final Metric (Dark Green)\n",
    "                \"#daa520\",  # Hanford (Goldenrod)\n",
    "                \"#ff6347\",  # Livingston (Tomato)\n",
    "            ]\n",
    "            # Strain plot\n",
    "            strain_ts = np.linspace(0, len(strain_chunks[j, 0, :]) / SAMPLE_RATE, len(strain_chunks[j, 0, :]))\n",
    "            axs1[2].plot(strain_ts, strain_chunks[j, 0, :], label='Hanford', alpha=0.8, c=\"#6c5b7b\")\n",
    "            axs1[2].plot(strain_ts, strain_chunks[j, 1, :], label='Livingston', alpha=0.8, c=\"#f29e4c\")\n",
    "            axs1[2].set_ylabel('Strain', fontsize=14)\n",
    "            axs1[2].legend()\n",
    "            # axs1[0].set_title(f'GPS time: {start_point + midpoints[loudest] / SAMPLE_RATE + hour_split * eval_at_once_len:.1f}')\n",
    "            # Remove x-axis ticks\n",
    "            axs1[2].tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "            axs1[2].set_xticklabels([])\n",
    "\n",
    "            # GWAK values plot\n",
    "            for i in range(scaled_evals.shape[1]):\n",
    "                line_type = \"-\" if i % 2 == 0 else \"--\"\n",
    "                axs1[3].plot(\n",
    "                    1000 * quak_evals_ts,\n",
    "                    scaled_evals[loudest - left_edge:loudest + right_edge, i],\n",
    "                    label=labels[i] if i % 2 == 0 or labels[i] in [\"Frequency correlation\"] else None,\n",
    "                    c=cols[i // 2],\n",
    "                    linestyle=line_type\n",
    "                )\n",
    "            axs1[3].plot(\n",
    "                1000 * quak_evals_ts,\n",
    "                smoothed_scores[loudest - left_edge:loudest + right_edge] - bias_value,\n",
    "                label='Final metric',\n",
    "                c='black'\n",
    "            )\n",
    "            axs1[3].set_xlabel(\"Time (ms)\", fontsize=14)\n",
    "            axs1[3].set_ylabel(\"Final metric contributions\", fontsize=14)\n",
    "            axs1[3].legend()\n",
    "                    \n",
    "            # Define a custom colormap with pink in the middle\n",
    "            custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#1f77b4\", \"#f4a3c1\", \"#ffd700\"], N=256)\n",
    "            \n",
    "            # Define shared color scale range and normalization (pink in the middle)\n",
    "            vmin, vcenter, vmax = 0, 12.5, 25  # Pink at 12.5, scale from 0 to 25\n",
    "            norm = TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n",
    "            \n",
    "            # Hanford and Livingston Q-Transforms\n",
    "            p = midpoints[loudest]\n",
    "            left_edge = 1024\n",
    "            right_edge = 1024\n",
    "            q_edge = int(7.5 * 4096)\n",
    "            \n",
    "            H_strain = gwpy_timeseries[0][\n",
    "                p - left_edge - q_edge + eval_at_once_len * hour_split * SAMPLE_RATE:\n",
    "                p + right_edge + q_edge + eval_at_once_len * hour_split * SAMPLE_RATE\n",
    "            ]\n",
    "            L_strain = gwpy_timeseries[1][\n",
    "                p - left_edge - q_edge + eval_at_once_len * hour_split * SAMPLE_RATE:\n",
    "                p + right_edge + q_edge + eval_at_once_len * hour_split * SAMPLE_RATE\n",
    "            ]\n",
    "            \n",
    "            t0 = H_strain.t0.value\n",
    "            dt = H_strain.dt.value\n",
    "            H_hq = H_strain.q_transform(outseg=(t0 + q_edge * dt, t0 + q_edge * dt + (left_edge + right_edge) * dt), whiten=False)\n",
    "            L_hq = L_strain.q_transform(outseg=(t0 + q_edge * dt, t0 + q_edge * dt + (left_edge + right_edge) * dt), whiten=False)\n",
    "            \n",
    "            f = np.array(H_hq.yindex)\n",
    "            t = np.array(H_hq.xindex)\n",
    "            t -= t[0]\n",
    "            \n",
    "            # Create the Q-Transform plots\n",
    "            im_H = axs1[0].pcolormesh(\n",
    "                t * 1000,\n",
    "                f,\n",
    "                np.array(H_hq).T,\n",
    "                cmap=custom_cmap,\n",
    "                norm=norm,\n",
    "                shading=\"auto\"\n",
    "            )\n",
    "            axs1[0].set_yscale(\"log\")\n",
    "            axs1[0].set_ylabel(\"Frequency (Hz)\", fontsize=14)\n",
    "            # axs1[1].set_title(\"Hanford Q-Transform\", fontsize=14)\n",
    "            axs1[1].tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "            axs1[0].tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "            axs1[1].set_xticklabels([])\n",
    "            axs1[0].set_xticklabels([])\n",
    "\n",
    "            \n",
    "            im_L = axs1[1].pcolormesh(\n",
    "                t * 1000,\n",
    "                f,\n",
    "                np.array(L_hq).T,\n",
    "                cmap=custom_cmap,\n",
    "                norm=norm,\n",
    "                shading=\"auto\"\n",
    "            )\n",
    "            axs1[1].set_yscale(\"log\")\n",
    "            # axs1[1].set_xlabel(\"Time (ms)\", fontsize=12)\n",
    "            axs1[1].set_ylabel(\"Frequency (Hz)\", fontsize=14)\n",
    "            # axs1[1].set_title(\"Livingston Q-Transform\", fontsize=14)\n",
    "            \n",
    "            # Add shared colorbar\n",
    "            cbar = fig1.colorbar(\n",
    "                im_H,\n",
    "                ax=axs1[0],\n",
    "                location=\"top\",\n",
    "                pad=0.05,\n",
    "                # shrink=0.9,\n",
    "                aspect=30\n",
    "            )\n",
    "            cbar.set_label(\"Spectral Power\", fontsize=12)\n",
    "            \n",
    "            # Adjust layout\n",
    "            fig1.tight_layout()\n",
    "            \n",
    "\n",
    "\n",
    "            # Save figures\n",
    "            base = f'{savedir}/{start_point + p / SAMPLE_RATE:.3f}_{fm_scores[j][0]:.2f}'\n",
    "            fig1.savefig(f'{base}_strain_gwak_values.png', dpi=300, bbox_inches=\"tight\")\n",
    "            # plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "            # fig2.savefig(f'{base}_q_transforms.png', dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close(fig1)\n",
    "            # plt.close(fig2)\n",
    "            \n",
    "            # Save data\n",
    "            np.savez(\n",
    "                f\"{base}.npz\",\n",
    "                {\n",
    "                    \"strain\": [strain_ts, strain_chunks[j]],\n",
    "                    \"gwak_values\": [1000 * quak_evals_ts, smoothed_scores[loudest - left_edge:loudest + right_edge] - bias_value],\n",
    "                    \"H_qtransform\": [t * 1000, f, np.array(H_hq).T],\n",
    "                    \"L_qtransform\": [t * 1000, f, np.array(L_hq).T],\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = 'plots/'\n",
    "try:\n",
    "    os.makedirs(savedir)\n",
    "except FileExistsError:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_start_times = [\n",
    "    1251009253.724, # Loudest BBH\n",
    "    # 1249529264.698, # Loudest non-BBH\n",
    "    1263013367.055, # Loudest Cat2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting, 1251009253\n",
      "N splits: 2\n",
      "0 14745600\n",
      "110 torch.Size([1, 2, 14745600])\n",
      "113 torch.Size([1, 2, 14745600])\n",
      "Data shape before torch.Size([1, 2, 14745600])\n",
      "Data shape after torch.Size([1, 2, 14745600])\n",
      "414 tensor(291204, device='cuda:1')\n",
      "433 together [-0.10580558  5.48293708  5.45512077  1.11369562  1.097615\n",
      "  1.18595445] [-6.2949834]\n",
      "0.5038995146751404 0.46108372383896157 [-2.9025142]\n",
      "433 together [-0.05383994  5.43422933  5.37065425  1.07209396  1.03651297\n",
      "  1.14701533] [5.4699736]\n",
      "0.5239849090576172 0.2769990694751556 [1.5151775]\n",
      "Fraction removed by heuristics test 1/2\n",
      "14745600 14745600\n",
      "starting, 1263013367\n",
      "N splits: 2\n",
      "0 14745600\n",
      "110 torch.Size([1, 2, 14745600])\n",
      "113 torch.Size([1, 2, 14745600])\n",
      "Data shape before torch.Size([1, 2, 14745600])\n",
      "Data shape after torch.Size([1, 2, 14745600])\n",
      "414 tensor(291204, device='cuda:1')\n",
      "433 together [-0.49343655  7.04205279  7.59011207  1.09150732  1.18234193\n",
      "  1.05331838] [-3.6543622]\n",
      "0.17070339620113373 0.999998096594781 [-3.6543553]\n",
      "433 together [-0.04965252  5.82422947  5.44635438  1.08937252  3.23385644\n",
      "  1.00918663] [-1.6900482]\n",
      "0.6153086423873901 0.009830895665337458 [-0.01661469]\n",
      "433 together [-0.04517945  5.45405652  5.40981295  1.05921602  1.01946139\n",
      "  1.08930004] [5.754877]\n",
      "0.5213881134986877 0.2982752448213284 [1.7165374]\n",
      "Fraction removed by heuristics test 2/3\n",
      "14745600 14745600\n"
     ]
    }
   ],
   "source": [
    "trained_path = \"/home/katya.govorkova/gwak-paper-final-models/trained/models/\"\n",
    "\n",
    "for A in anomaly_start_times:\n",
    "    A = int(float(A))\n",
    "    B = A + 3600\n",
    "    print(\"starting,\", A)\n",
    "    H, L = whiten_bandpass_resample_new_order(A, B)\n",
    "\n",
    "    Hclean, Lclean = whiten_bandpass_resample(A, B)\n",
    "    data = np.vstack([np.array(H.data), np.array(L.data)])\n",
    "\n",
    "    base = 3554.75\n",
    "    get_evals(data, trained_path, savedir, int(A), [Hclean, Lclean],\n",
    "              manual_eval_times=[base])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
